{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import timedelta\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from open_data import open_data, create_global_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CSV=\"/Users/clemencevast/Documents/Courses/MachineLearning/RadarTrafficData_files/Radar_Traffic_Counts.csv\" #\"/Users/iris/Documents/radar_deep/Radar_Traffic_Counts.csv\"\n",
    "radar_name=' CAPITAL OF TEXAS HWY / LAKEWOOD DR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=open_data(PATH_CSV, direction=\"NB\", radar=radar_name, year=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building batch 0 \n",
      " x begin 2018-01-02 label begin 2018-01-08 end period 2018-01-30 \n",
      "Building batch 1 \n",
      " x begin 2018-01-03 label begin 2018-01-09 end period 2018-01-30 \n",
      "Building batch 2 \n",
      " x begin 2018-01-04 label begin 2018-01-10 end period 2018-01-30 \n",
      "Building batch 3 \n",
      " x begin 2018-01-05 label begin 2018-01-11 end period 2018-01-30 \n",
      "Building batch 4 \n",
      " x begin 2018-01-06 label begin 2018-01-12 end period 2018-01-30 \n",
      "Building batch 5 \n",
      " x begin 2018-01-07 label begin 2018-01-13 end period 2018-01-30 \n",
      "We do not have all the dates for the time period in label , 1 []\n",
      "Building batch 5 \n",
      " x begin 2018-01-08 label begin 2018-01-14 end period 2018-01-30 \n",
      "We do not have all the dates for the time period in x , 6 [datetime.date(2018, 1, 8) datetime.date(2018, 1, 9)\n",
      " datetime.date(2018, 1, 10) datetime.date(2018, 1, 11)\n",
      " datetime.date(2018, 1, 12)]\n",
      "Building batch 5 \n",
      " x begin 2018-01-09 label begin 2018-01-15 end period 2018-01-30 \n",
      "We do not have all the dates for the time period in x , 6 [datetime.date(2018, 1, 9) datetime.date(2018, 1, 10)\n",
      " datetime.date(2018, 1, 11) datetime.date(2018, 1, 12)]\n",
      "Building batch 5 \n",
      " x begin 2018-01-10 label begin 2018-01-16 end period 2018-01-30 \n",
      "We do not have all the dates for the time period in x , 6 [datetime.date(2018, 1, 10) datetime.date(2018, 1, 11)\n",
      " datetime.date(2018, 1, 12)]\n",
      "Building batch 5 \n",
      " x begin 2018-01-11 label begin 2018-01-17 end period 2018-01-30 \n",
      "We do not have all the dates for the time period in x , 6 [datetime.date(2018, 1, 11) datetime.date(2018, 1, 12)\n",
      " datetime.date(2018, 1, 16)]\n",
      "Building batch 5 \n",
      " x begin 2018-01-12 label begin 2018-01-18 end period 2018-01-30 \n",
      "We do not have all the dates for the time period in x , 6 [datetime.date(2018, 1, 12) datetime.date(2018, 1, 16)\n",
      " datetime.date(2018, 1, 17)]\n",
      "Building batch 5 \n",
      " x begin 2018-01-13 label begin 2018-01-19 end period 2018-01-30 \n",
      "We do not have all the dates for the time period in x , 6 [datetime.date(2018, 1, 16) datetime.date(2018, 1, 17)\n",
      " datetime.date(2018, 1, 18)]\n",
      "Building batch 5 \n",
      " x begin 2018-01-14 label begin 2018-01-20 end period 2018-01-30 \n",
      "We do not have all the dates for the time period in x , 6 [datetime.date(2018, 1, 16) datetime.date(2018, 1, 17)\n",
      " datetime.date(2018, 1, 18) datetime.date(2018, 1, 19)]\n",
      "Building batch 5 \n",
      " x begin 2018-01-15 label begin 2018-01-21 end period 2018-01-30 \n",
      "We do not have all the dates for the time period in x , 6 [datetime.date(2018, 1, 16) datetime.date(2018, 1, 17)\n",
      " datetime.date(2018, 1, 18) datetime.date(2018, 1, 19)\n",
      " datetime.date(2018, 1, 20)]\n",
      "Building batch 5 \n",
      " x begin 2018-01-16 label begin 2018-01-22 end period 2018-01-30 \n",
      "Building batch 6 \n",
      " x begin 2018-01-17 label begin 2018-01-23 end period 2018-01-30 \n",
      "Building batch 7 \n",
      " x begin 2018-01-18 label begin 2018-01-24 end period 2018-01-30 \n",
      "Building batch 8 \n",
      " x begin 2018-01-19 label begin 2018-01-25 end period 2018-01-30 \n",
      "Building batch 9 \n",
      " x begin 2018-01-20 label begin 2018-01-26 end period 2018-01-30 \n",
      "Building batch 10 \n",
      " x begin 2018-01-21 label begin 2018-01-27 end period 2018-01-30 \n",
      "Building batch 11 \n",
      " x begin 2018-01-22 label begin 2018-01-28 end period 2018-01-30 \n",
      "Building batch 12 \n",
      " x begin 2018-01-23 label begin 2018-01-29 end period 2018-01-30 \n",
      "Building batch 13 \n",
      " x begin 2018-01-24 label begin 2018-01-30 end period 2018-01-30 \n",
      "Building batch 14 \n",
      " x begin 2018-01-25 label begin 2018-01-31 end period 2018-01-30 \n",
      "Building batch 15 \n",
      " x begin 2018-01-26 label begin 2018-02-01 end period 2018-01-30 \n",
      "Building batch 16 \n",
      " x begin 2018-01-27 label begin 2018-02-02 end period 2018-01-30 \n",
      "Building batch 17 \n",
      " x begin 2018-01-28 label begin 2018-02-03 end period 2018-01-30 \n",
      "Building batch 18 \n",
      " x begin 2018-01-29 label begin 2018-02-04 end period 2018-01-30 \n"
     ]
    }
   ],
   "source": [
    "batch_df=create_global_batch(df1, window_x_day=6, window_label_day=1, gap_acquisition=1, tot_len_day=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vol_data_x</th>\n",
       "      <th>vol_label_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[22, 13, 21, 21, 15, 11, 19, 5, 14, 6, 5, 8, 4...</td>\n",
       "      <td>[24, 21, 18, 10, 14, 10, 9, 12, 7, 10, 9, 9, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[33, 31, 20, 22, 13, 13, 13, 9, 3, 13, 4, 6, 1...</td>\n",
       "      <td>[66, 40, 40, 22, 28, 22, 13, 9, 10, 14, 17, 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[32, 31, 25, 29, 29, 10, 21, 17, 13, 13, 5, 8,...</td>\n",
       "      <td>[36, 32, 28, 26, 23, 20, 14, 21, 8, 9, 11, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[33, 37, 28, 19, 26, 16, 13, 14, 14, 5, 8, 6, ...</td>\n",
       "      <td>[48, 32, 36, 28, 32, 23, 8, 22, 17, 7, 21, 15,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[59, 57, 56, 46, 30, 32, 19, 16, 19, 14, 17, 1...</td>\n",
       "      <td>[54, 36, 34, 30, 28, 19, 26, 17, 10, 8, 13, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[100, 101, 95, 95, 109, 83, 91, 82, 124, 109, ...</td>\n",
       "      <td>[24, 26, 16, 17, 10, 8, 10, 11, 11, 10, 10, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[13, 5, 14, 15, 9, 4, 6, 1, 4, 1, 5, 2, 4, 6, ...</td>\n",
       "      <td>[33, 38, 45, 30, 25, 10, 13, 15, 4, 8, 4, 9, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[34, 29, 28, 26, 17, 14, 15, 5, 6, 11, 8, 7, 5...</td>\n",
       "      <td>[38, 38, 26, 21, 26, 22, 18, 11, 2, 4, 8, 6, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[32, 25, 20, 25, 20, 19, 18, 8, 6, 12, 10, 9, ...</td>\n",
       "      <td>[35, 35, 35, 23, 23, 14, 13, 13, 8, 6, 9, 9, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[56, 54, 41, 39, 27, 28, 32, 17, 18, 12, 18, 1...</td>\n",
       "      <td>[38, 34, 32, 23, 29, 15, 22, 16, 12, 12, 20, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[75, 62, 49, 42, 42, 44, 27, 23, 20, 18, 14, 2...</td>\n",
       "      <td>[83, 64, 45, 47, 29, 37, 26, 24, 33, 18, 12, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[24, 26, 16, 17, 10, 8, 10, 11, 11, 10, 10, 5,...</td>\n",
       "      <td>[80, 65, 59, 64, 42, 37, 32, 27, 34, 20, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[33, 38, 45, 30, 25, 10, 13, 15, 4, 8, 4, 9, 1...</td>\n",
       "      <td>[35, 24, 26, 20, 22, 15, 11, 12, 5, 14, 7, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[38, 38, 26, 21, 26, 22, 18, 11, 2, 4, 8, 6, 1...</td>\n",
       "      <td>[37, 25, 26, 30, 12, 25, 12, 10, 8, 6, 10, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[35, 35, 35, 23, 23, 14, 13, 13, 8, 6, 9, 9, 6...</td>\n",
       "      <td>[29, 34, 20, 25, 15, 10, 13, 12, 8, 13, 6, 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[38, 34, 32, 23, 29, 15, 22, 16, 12, 12, 20, 9...</td>\n",
       "      <td>[47, 26, 36, 14, 19, 14, 16, 12, 13, 8, 7, 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[83, 64, 45, 47, 29, 37, 26, 24, 33, 18, 12, 1...</td>\n",
       "      <td>[32, 36, 21, 33, 23, 15, 8, 10, 7, 12, 12, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[80, 65, 59, 64, 42, 37, 32, 27, 34, 20, 13, 1...</td>\n",
       "      <td>[65, 58, 57, 40, 49, 47, 31, 23, 20, 19, 14, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[35, 24, 26, 20, 22, 15, 11, 12, 5, 14, 7, 4, ...</td>\n",
       "      <td>[78, 49, 63, 65, 40, 45, 33, 31, 21, 15, 13, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           vol_data_x  \\\n",
       "0   [22, 13, 21, 21, 15, 11, 19, 5, 14, 6, 5, 8, 4...   \n",
       "1   [33, 31, 20, 22, 13, 13, 13, 9, 3, 13, 4, 6, 1...   \n",
       "2   [32, 31, 25, 29, 29, 10, 21, 17, 13, 13, 5, 8,...   \n",
       "3   [33, 37, 28, 19, 26, 16, 13, 14, 14, 5, 8, 6, ...   \n",
       "4   [59, 57, 56, 46, 30, 32, 19, 16, 19, 14, 17, 1...   \n",
       "5   [100, 101, 95, 95, 109, 83, 91, 82, 124, 109, ...   \n",
       "6   [13, 5, 14, 15, 9, 4, 6, 1, 4, 1, 5, 2, 4, 6, ...   \n",
       "7   [34, 29, 28, 26, 17, 14, 15, 5, 6, 11, 8, 7, 5...   \n",
       "8   [32, 25, 20, 25, 20, 19, 18, 8, 6, 12, 10, 9, ...   \n",
       "9   [56, 54, 41, 39, 27, 28, 32, 17, 18, 12, 18, 1...   \n",
       "10  [75, 62, 49, 42, 42, 44, 27, 23, 20, 18, 14, 2...   \n",
       "11  [24, 26, 16, 17, 10, 8, 10, 11, 11, 10, 10, 5,...   \n",
       "12  [33, 38, 45, 30, 25, 10, 13, 15, 4, 8, 4, 9, 1...   \n",
       "13  [38, 38, 26, 21, 26, 22, 18, 11, 2, 4, 8, 6, 1...   \n",
       "14  [35, 35, 35, 23, 23, 14, 13, 13, 8, 6, 9, 9, 6...   \n",
       "15  [38, 34, 32, 23, 29, 15, 22, 16, 12, 12, 20, 9...   \n",
       "16  [83, 64, 45, 47, 29, 37, 26, 24, 33, 18, 12, 1...   \n",
       "17  [80, 65, 59, 64, 42, 37, 32, 27, 34, 20, 13, 1...   \n",
       "18  [35, 24, 26, 20, 22, 15, 11, 12, 5, 14, 7, 4, ...   \n",
       "\n",
       "                                          vol_label_y  \n",
       "0   [24, 21, 18, 10, 14, 10, 9, 12, 7, 10, 9, 9, 5...  \n",
       "1   [66, 40, 40, 22, 28, 22, 13, 9, 10, 14, 17, 7,...  \n",
       "2   [36, 32, 28, 26, 23, 20, 14, 21, 8, 9, 11, 9, ...  \n",
       "3   [48, 32, 36, 28, 32, 23, 8, 22, 17, 7, 21, 15,...  \n",
       "4   [54, 36, 34, 30, 28, 19, 26, 17, 10, 8, 13, 14...  \n",
       "5   [24, 26, 16, 17, 10, 8, 10, 11, 11, 10, 10, 5,...  \n",
       "6   [33, 38, 45, 30, 25, 10, 13, 15, 4, 8, 4, 9, 1...  \n",
       "7   [38, 38, 26, 21, 26, 22, 18, 11, 2, 4, 8, 6, 1...  \n",
       "8   [35, 35, 35, 23, 23, 14, 13, 13, 8, 6, 9, 9, 6...  \n",
       "9   [38, 34, 32, 23, 29, 15, 22, 16, 12, 12, 20, 9...  \n",
       "10  [83, 64, 45, 47, 29, 37, 26, 24, 33, 18, 12, 1...  \n",
       "11  [80, 65, 59, 64, 42, 37, 32, 27, 34, 20, 13, 1...  \n",
       "12  [35, 24, 26, 20, 22, 15, 11, 12, 5, 14, 7, 4, ...  \n",
       "13  [37, 25, 26, 30, 12, 25, 12, 10, 8, 6, 10, 8, ...  \n",
       "14  [29, 34, 20, 25, 15, 10, 13, 12, 8, 13, 6, 10,...  \n",
       "15  [47, 26, 36, 14, 19, 14, 16, 12, 13, 8, 7, 12,...  \n",
       "16  [32, 36, 21, 33, 23, 15, 8, 10, 7, 12, 12, 5, ...  \n",
       "17  [65, 58, 57, 40, 49, 47, 31, 23, 20, 19, 14, 1...  \n",
       "18  [78, 49, 63, 65, 40, 45, 33, 31, 21, 15, 13, 1...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe[\"vol_data_x\"].values\n",
    "        self.target = dataframe[\"vol_label_y\"].values\n",
    "        \n",
    "    def __len__(self): # total number of samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index): # how to preprocess the files (index = iterator on samples)\n",
    "        sample = self.data[index]\n",
    "        label = self.target[index]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarCollate(object):\n",
    "    \"\"\"Function object used as a collate function for DataLoader.\"\"\"\n",
    "\n",
    "    def __init__(self,):\n",
    "        pass\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        data_list, label_list = [], []\n",
    "        for _data, _label in batch:\n",
    "            data_list.append(_data)\n",
    "            label_list.append(_label)\n",
    "        return torch.Tensor(data_list), torch.LongTensor(label_list)\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        return self._collate_fn(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_rate = 16000\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset = RadarDataset(dataframe=batch_df)\n",
    "traindataset, testdataset = train_test_split(fullset, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = RadarCollate()\n",
    "trainloader = DataLoader(traindataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "testloader = DataLoader(testdataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch number 0\n",
      "torch.Size([1, 574]) torch.Size([1, 96]) \n",
      "\n",
      "batch number 1\n",
      "torch.Size([1, 574]) torch.Size([1, 94]) \n",
      "\n",
      "batch number 2\n",
      "torch.Size([1, 574]) torch.Size([1, 96]) \n",
      "\n",
      "batch number 3\n",
      "torch.Size([1, 572]) torch.Size([1, 96]) \n",
      "\n",
      "batch number 4\n",
      "torch.Size([1, 574]) torch.Size([1, 96]) \n",
      "\n",
      "batch number 5\n",
      "torch.Size([1, 521]) torch.Size([1, 94]) \n",
      "\n",
      "batch number 6\n",
      "torch.Size([1, 574]) torch.Size([1, 96]) \n",
      "\n",
      "batch number 7\n",
      "torch.Size([1, 574]) torch.Size([1, 96]) \n",
      "\n",
      "batch number 8\n",
      "torch.Size([1, 574]) torch.Size([1, 96]) \n",
      "\n",
      "batch number 9\n",
      "torch.Size([1, 574]) torch.Size([1, 96]) \n",
      "\n",
      "batch number 10\n",
      "torch.Size([1, 572]) torch.Size([1, 96]) \n",
      "\n",
      "batch number 11\n",
      "torch.Size([1, 576]) torch.Size([1, 94]) \n",
      "\n",
      "batch number 12\n",
      "torch.Size([1, 574]) torch.Size([1, 96]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "c= 0\n",
    "for x in trainloader:\n",
    "    print(\"batch number {}\".format(c))\n",
    "    print(x[0].shape, x[1].shape, \"\\n\")\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO  :  NOT THE SAME INPUT SIZE ISSUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 574    \n",
    "hidden_dim = 256\n",
    "layer_dim = 2\n",
    "output_dim = 96\n",
    "seq_dim = 1\n",
    "\n",
    "lr = 0.05\n",
    "n_epochs = 10\n",
    "iterations_per_epoch = len(trainloader)\n",
    "best_acc = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=574, hidden_layer_size=100, output_size=96):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clemencevast/opt/anaconda3/envs/dl_env/lib/python3.7/site-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([1, 96])) that is different to the input size (torch.Size([96])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float\nException raised from compute_types at /tmp/pip-req-build-rc66hrpz/aten/src/ATen/native/TensorIterator.cpp:183 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) + 183 (0x104245227 in libc10.dylib)\nframe #1: at::TensorIterator::compute_types(at::TensorIteratorConfig const&) + 4390 (0x111535316 in libtorch_cpu.dylib)\nframe #2: at::TensorIterator::build(at::TensorIteratorConfig&) + 596 (0x11153e914 in libtorch_cpu.dylib)\nframe #3: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 210 (0x11153e612 in libtorch_cpu.dylib)\nframe #4: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 438 (0x111374da6 in libtorch_cpu.dylib)\nframe #5: at::CPUType::mse_loss_backward_out_grad_input(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 9 (0x1117b3a69 in libtorch_cpu.dylib)\nframe #6: at::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 157 (0x111876d8d in libtorch_cpu.dylib)\nframe #7: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 134 (0x111374af6 in libtorch_cpu.dylib)\nframe #8: at::CPUType::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 14 (0x1117b3a7e in libtorch_cpu.dylib)\nframe #9: c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, at::Tensor const&, long long> >, at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 27 (0x1111334cb in libtorch_cpu.dylib)\nframe #10: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)> const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) const + 291 (0x1118a8303 in libtorch_cpu.dylib)\nframe #11: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 157 (0x111876e5d in libtorch_cpu.dylib)\nframe #12: torch::autograd::VariableType::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 794 (0x1138e257a in libtorch_cpu.dylib)\nframe #13: c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, at::Tensor const&, long long> >, at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 27 (0x1111334cb in libtorch_cpu.dylib)\nframe #14: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)> const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) const + 291 (0x1118a8303 in libtorch_cpu.dylib)\nframe #15: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 157 (0x111876e5d in libtorch_cpu.dylib)\nframe #16: torch::autograd::generated::MseLossBackward::apply(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&) + 246 (0x11376e5f6 in libtorch_cpu.dylib)\nframe #17: torch::autograd::Node::operator()(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&) + 761 (0x113f08a39 in libtorch_cpu.dylib)\nframe #18: torch::autograd::Engine::evaluate_function(std::__1::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::__1::shared_ptr<torch::autograd::ReadyQueue> const&) + 1918 (0x113efdd2e in libtorch_cpu.dylib)\nframe #19: torch::autograd::Engine::thread_main(std::__1::shared_ptr<torch::autograd::GraphTask> const&) + 805 (0x113efcc15 in libtorch_cpu.dylib)\nframe #20: torch::autograd::Engine::execute_with_graph_task(std::__1::shared_ptr<torch::autograd::GraphTask> const&, std::__1::shared_ptr<torch::autograd::Node>) + 1002 (0x113f06c2a in libtorch_cpu.dylib)\nframe #21: torch::autograd::python::PythonEngine::execute_with_graph_task(std::__1::shared_ptr<torch::autograd::GraphTask> const&, std::__1::shared_ptr<torch::autograd::Node>) + 79 (0x11051d44f in libtorch_python.dylib)\nframe #22: torch::autograd::Engine::execute(std::__1::vector<torch::autograd::Edge, std::__1::allocator<torch::autograd::Edge> > const&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> > const&, bool, bool, std::__1::vector<torch::autograd::Edge, std::__1::allocator<torch::autograd::Edge> > const&) + 647 (0x113f05307 in libtorch_cpu.dylib)\nframe #23: torch::autograd::python::PythonEngine::execute(std::__1::vector<torch::autograd::Edge, std::__1::allocator<torch::autograd::Edge> > const&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> > const&, bool, bool, std::__1::vector<torch::autograd::Edge, std::__1::allocator<torch::autograd::Edge> > const&) + 102 (0x11051d226 in libtorch_python.dylib)\nframe #24: THPEngine_run_backward(THPEngine*, _object*, _object*) + 2062 (0x11051dd4e in libtorch_python.dylib)\n<omitting python frames>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-4a014a1cf353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msingle_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msingle_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float\nException raised from compute_types at /tmp/pip-req-build-rc66hrpz/aten/src/ATen/native/TensorIterator.cpp:183 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) + 183 (0x104245227 in libc10.dylib)\nframe #1: at::TensorIterator::compute_types(at::TensorIteratorConfig const&) + 4390 (0x111535316 in libtorch_cpu.dylib)\nframe #2: at::TensorIterator::build(at::TensorIteratorConfig&) + 596 (0x11153e914 in libtorch_cpu.dylib)\nframe #3: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 210 (0x11153e612 in libtorch_cpu.dylib)\nframe #4: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 438 (0x111374da6 in libtorch_cpu.dylib)\nframe #5: at::CPUType::mse_loss_backward_out_grad_input(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 9 (0x1117b3a69 in libtorch_cpu.dylib)\nframe #6: at::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 157 (0x111876d8d in libtorch_cpu.dylib)\nframe #7: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 134 (0x111374af6 in libtorch_cpu.dylib)\nframe #8: at::CPUType::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 14 (0x1117b3a7e in libtorch_cpu.dylib)\nframe #9: c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, at::Tensor const&, long long> >, at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 27 (0x1111334cb in libtorch_cpu.dylib)\nframe #10: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)> const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) const + 291 (0x1118a8303 in libtorch_cpu.dylib)\nframe #11: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 157 (0x111876e5d in libtorch_cpu.dylib)\nframe #12: torch::autograd::VariableType::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 794 (0x1138e257a in libtorch_cpu.dylib)\nframe #13: c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, at::Tensor const&, long long> >, at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 27 (0x1111334cb in libtorch_cpu.dylib)\nframe #14: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)> const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) const + 291 (0x1118a8303 in libtorch_cpu.dylib)\nframe #15: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 157 (0x111876e5d in libtorch_cpu.dylib)\nframe #16: torch::autograd::generated::MseLossBackward::apply(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&) + 246 (0x11376e5f6 in libtorch_cpu.dylib)\nframe #17: torch::autograd::Node::operator()(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&) + 761 (0x113f08a39 in libtorch_cpu.dylib)\nframe #18: torch::autograd::Engine::evaluate_function(std::__1::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::__1::shared_ptr<torch::autograd::ReadyQueue> const&) + 1918 (0x113efdd2e in libtorch_cpu.dylib)\nframe #19: torch::autograd::Engine::thread_main(std::__1::shared_ptr<torch::autograd::GraphTask> const&) + 805 (0x113efcc15 in libtorch_cpu.dylib)\nframe #20: torch::autograd::Engine::execute_with_graph_task(std::__1::shared_ptr<torch::autograd::GraphTask> const&, std::__1::shared_ptr<torch::autograd::Node>) + 1002 (0x113f06c2a in libtorch_cpu.dylib)\nframe #21: torch::autograd::python::PythonEngine::execute_with_graph_task(std::__1::shared_ptr<torch::autograd::GraphTask> const&, std::__1::shared_ptr<torch::autograd::Node>) + 79 (0x11051d44f in libtorch_python.dylib)\nframe #22: torch::autograd::Engine::execute(std::__1::vector<torch::autograd::Edge, std::__1::allocator<torch::autograd::Edge> > const&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> > const&, bool, bool, std::__1::vector<torch::autograd::Edge, std::__1::allocator<torch::autograd::Edge> > const&) + 647 (0x113f05307 in libtorch_cpu.dylib)\nframe #23: torch::autograd::python::PythonEngine::execute(std::__1::vector<torch::autograd::Edge, std::__1::allocator<torch::autograd::Edge> > const&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> > const&, bool, bool, std::__1::vector<torch::autograd::Edge, std::__1::allocator<torch::autograd::Edge> > const&) + 102 (0x11051d226 in libtorch_python.dylib)\nframe #24: THPEngine_run_backward(THPEngine*, _object*, _object*) + 2062 (0x11051dd4e in libtorch_python.dylib)\n<omitting python frames>\n"
     ]
    }
   ],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
